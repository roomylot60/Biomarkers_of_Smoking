{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6957118a",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7418f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20593359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #for data manipulation operations\n",
    "import numpy as np  #for numeric operations on data\n",
    "import seaborn as sns  #for data visualization operations\n",
    "import matplotlib.pyplot as plt  #for data visualization operations\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # for encoding\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler #for standardization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dddd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking = pd.read_csv(\"../data/Smoking_raw/smoking.csv\")\n",
    "smoking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56738523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show all columns\n",
    "pd.set_option('display.max_columns', 30)\n",
    "desc = smoking.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd97e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.T[['min','max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThere are totally {} null values in the dataset\".format(smoking.isnull().sum().sum())) # for checking null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df_scaling):\n",
    "    # 불필요한 컬럼 제거\n",
    "    if 'ID' in df_scaling:\n",
    "        df_scaling = df_scaling.drop(\"ID\", axis = 1)\n",
    "        if len(df_scaling.columns) == 1:\n",
    "            return df_scaling\n",
    "        \n",
    "        # Oral(=구강검사 여부) 특성값은 모두 Y 값이므로 삭제.\n",
    "        df_scaling = df_scaling.drop('oral', axis = 1) \n",
    "        \n",
    "        # 범주형 피처 레이블 인코딩 \n",
    "        cate_features = df_scaling[['gender','tartar']]\n",
    "\n",
    "        lbe = LabelEncoder()\n",
    "        lbe.fit_transform(df_scaling[\"gender\"])\n",
    "        df_scaling[\"gender\"] = lbe.fit_transform(df_scaling[\"gender\"])\n",
    "\n",
    "        lbe = LabelEncoder()\n",
    "        lbe.fit_transform(df_scaling[\"tartar\"])\n",
    "        df_scaling[\"tartar\"] = lbe.fit_transform(df_scaling[\"tartar\"])\n",
    "        \n",
    "        # hearing 피처 1, 2 => 1, 0으로 변환\n",
    "        df_scaling['hearing(left)'] = df_scaling['hearing(left)'].apply(lambda x: x-2 if x == 2.0 else x )\n",
    "        df_scaling['hearing(right)'] = df_scaling['hearing(right)'].apply(lambda x: x-2 if x == 2.0 else x )\n",
    "        \n",
    "        # BMI 지수 계산 : bmi = kg/m^2\n",
    "        df_scaling['bmi'] = df_scaling['weight(kg)']/((df_scaling['height(cm)']*0.01)**2)\n",
    "        # wwi(비만 지수) 지수 계산 : wwi = cm/sqrt(kg)\n",
    "        df_scaling['wwi'] = df_scaling['waist(cm)']/(df_scaling['weight(kg)'].apply(np.sqrt))\n",
    "\n",
    "    return df_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fe194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(train_data, test_data, scaled_form = 'MinMaxScaler()'):\n",
    "    # 형태 별 특성 구분\n",
    "    train_data = preprocessing(train_data)\n",
    "    test_data = preprocessing(test_data)\n",
    "    tr_cate_features = train_data[['gender', 'tartar', 'hearing(right)', 'hearing(left)', 'dental caries']]\n",
    "    tr_scaled_features = train_data.drop(tr_cate_features.columns, axis=1)\n",
    "    \n",
    "    ts_cate_features = test_data[['gender', 'tartar', 'hearing(right)', 'hearing(left)', 'dental caries']]\n",
    "    ts_scaled_features = test_data.drop(ts_cate_features.columns, axis=1)\n",
    "    \n",
    "    if scaled_form == 'StandardScaler()':\n",
    "        # Standard scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tr_scaled_features) # 훈련 데이터에 fit() 적용\n",
    "        \n",
    "        # 훈련 데이터와 테스트 데이터에 transform()을 통해 변환\n",
    "        tr_scaled = scaler.transform(tr_scaled_features)\n",
    "        ts_scaled = scaler.transform(ts_scaled_features)\n",
    "        \n",
    "        train_std_scaled = pd.DataFrame(tr_scaled, columns=tr_scaled_features.columns)\n",
    "        train_std_scaled[tr_cate_features.columns] = tr_cate_features\n",
    "        \n",
    "        test_std_scaled = pd.DataFrame(ts_scaled, columns=ts_scaled_features.columns)\n",
    "        test_std_scaled[ts_cate_features.columns] = ts_cate_features\n",
    "        \n",
    "        return train_std_scaled, test_std_scaled\n",
    "    \n",
    "    elif scaled_form == 'RobustScaler()':\n",
    "        # Robust scaler\n",
    "        scaler = RobustScaler()\n",
    "        scaler.fit(tr_scaled_features) # 훈련 데이터에 fit() 적용\n",
    "        \n",
    "        # 훈련 데이터와 테스트 데이터에 transform()을 통해 변환\n",
    "        tr_scaled = scaler.transform(tr_scaled_features)\n",
    "        ts_scaled = scaler.transform(ts_scaled_features)\n",
    "        \n",
    "        # 데이터 프레임 형태로 변환\n",
    "        train_robust_scaled = pd.DataFrame(tr_scaled, columns=tr_scaled_features.columns)\n",
    "        train_robust_scaled[tr_cate_features.columns] = tr_cate_features\n",
    "        \n",
    "        test_robust_scaled = pd.DataFrame(ts_scaled, columns=ts_scaled_features.columns)\n",
    "        test_robust_scaled[ts_cate_features.columns] = ts_cate_features\n",
    "        \n",
    "        return train_robust_scaled, test_robust_scaled\n",
    "        \n",
    "    else:\n",
    "        # MinMax scaler\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(tr_scaled_features) # 훈련 데이터에 fit() 적용\n",
    "        \n",
    "        # 훈련 데이터와 테스트 데이터에 transform()을 통해 변환\n",
    "        tr_scaled = scaler.transform(tr_scaled_features)\n",
    "        ts_scaled = scaler.transform(ts_scaled_features)\n",
    "        \n",
    "        # 데이터 프레임 형태로 변환\n",
    "        train_mmx_scaled = pd.DataFrame(tr_scaled, columns = tr_scaled_features.columns)\n",
    "        train_mmx_scaled[tr_cate_features.columns] = tr_cate_features\n",
    "        \n",
    "        test_mmx_scaled = pd.DataFrame(ts_scaled,columns = ts_scaled_features.columns)\n",
    "        test_mmx_scaled[ts_cate_features.columns] = ts_cate_features\n",
    "\n",
    "        return train_mmx_scaled, test_mmx_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d95bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(smoking)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().style.background_gradient(cmap = \"magma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760d887",
   "metadata": {},
   "source": [
    "- gender, hemoglobin, height(cm), weight(cm), triglyceride, Gtp, waist(cm), serum creatinine 가 0.2 이상의 상관계수 값을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed32b73",
   "metadata": {},
   "source": [
    "### Competition Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9640b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_format\n",
    "x_train = pd.read_csv('../data/Smoking_raw/competition_format/x_train.csv')\n",
    "x_test = pd.read_csv('../data/Smoking_raw/competition_format/x_test.csv')\n",
    "y_train = pd.read_csv('../data/Smoking_raw/competition_format/y_train.csv')\n",
    "y_test = pd.read_csv('../data/Smoking_raw/competition_format/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca17156",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing(x_train)\n",
    "x_test = preprocessing(x_test)\n",
    "y_train = preprocessing(y_train)\n",
    "y_test = preprocessing(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = scaling(x_train, x_test, 'StandardScaler()')\n",
    "# x_train, x_test = scaling(x_train, x_test, 'RobustScaler()')\n",
    "# x_train, x_test = scaling(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd67e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f559f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data = df, x = \"age\", hue = \"smoking\",\n",
    "    kind = \"hist\", height = 5, aspect = 1.5,\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.50\").set(title =  \"density relationship between 'age' and 'smoking' variables\");\n",
    "\n",
    "\n",
    "sns.displot(\n",
    "    data = df, x = \"systolic\", hue = \"smoking\",\n",
    "    kind = \"kde\", height = 5, aspect = 1.5,\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.50\").set(title = \"density relationship between 'systolic' and 'smoking' variables\");\n",
    "\n",
    "\n",
    "sns.displot(\n",
    "    data = df, x = \"waist(cm)\", hue = \"smoking\",\n",
    "    kind = \"kde\", height = 5, aspect = 1.5, multiple=\"fill\",\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.50\").set(title = \"density relationship between 'waist(cm)' and 'smoking' variables\");\n",
    "\n",
    "sns.displot(\n",
    "    data = df, x = \"bmi\", hue = \"smoking\",\n",
    "    kind = \"kde\", height = 5, aspect = 1.5,\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.50\").set(title = \"density relationship between 'bmi' and 'smoking' variables\");\n",
    "\n",
    "sns.displot(\n",
    "    data = df, x = \"wwi\", hue = \"smoking\",\n",
    "    kind = \"kde\", height = 5, aspect = 1.5,\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.50\").set(title = \"density relationship between 'wwi' and 'smoking' variables\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec44e",
   "metadata": {},
   "source": [
    "## 2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db637bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features (estimator)\n",
    "x = df.drop(\"smoking\", axis = 1)\n",
    "\n",
    "# target (label)\n",
    "y = df[\"smoking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "#                                                     test_size = 0.1,\n",
    "#                                                     shuffle = True,\n",
    "#                                                     random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3198b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "dtrain = xgb.DMatrix(data=x_train,label=y_train,feature_names=x_train.columns)\n",
    "dtest = xgb.DMatrix(data=x_test,label=y_test,feature_names=x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa240480",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max-depth':3,\n",
    "          'eta':0.1, # XGBClassifier일 경우 learning_rate 로 쓴다\n",
    "          'objective':'binary:logistic', #목적함수 : 0 or 1 이므로 이진 로지스틱 사용\n",
    "          'eval_metric' : 'logloss', # 오류 함수의 평가 성능 지표 : logloss\n",
    "          'early_stoppings':100, #100회이상 시행시에도 오류가 내려가지않으면 중단\n",
    "          'silent' : 0,\n",
    "          'verbosity':0\n",
    "} # 트리 깊이 최대 3 , 학습률 0.1 , \n",
    "num_rounds = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b81f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlist = [(dtrain,'train'),(dtest,'eval')]\n",
    "start = time.time()\n",
    "xgb_model = xgb.train(params = params, dtrain=dtrain,num_boost_round= num_rounds,\n",
    "                      early_stopping_rounds=100,evals = wlist)\n",
    "end = time.time()\n",
    "print(\"XGB 수행 시간: {0:.1f} 초 \".format(end - start))\n",
    "pred_probs = xgb_model.predict(dtest)\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test,pred)\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test,pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce878da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,recall_score,precision_score\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test.values, pred_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='red',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73203868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,12)) # 축 반환\n",
    "plot_importance(xgb_model,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1873e65",
   "metadata": {},
   "source": [
    "#### Grid Search를 이용한 최적 하이퍼 파라미터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# xgb 모델 생성\n",
    "xgb = XGBClassifier(learning_rate=0.1, \n",
    "                    n_estimators=1000,\n",
    "                    gamma=0, \n",
    "                    subsample=0.8, # 각 트리마다의 관측 데이터 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    colsample_bytree=0.8, # 각 트리마다의 feature 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    objective= 'binary:logistic', \n",
    "                    verbose=10)\n",
    "\n",
    "# parameter 들을 dictionary 형태로 설정\n",
    "xgb_params = { \n",
    "              'max_depth':range(3,13,3), \n",
    "              'min_child_weight':range(1,6,2)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# GridSearch를 통해 최적 hyperparameter를 검색\n",
    "clf = GridSearchCV(xgb,xgb_params,scoring='accuracy',cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "pred = clf.predict(x_test)\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,pred)))\n",
    "end = time.time()\n",
    "print(\"XGB 수행 시간: {0:.1f} 초 \".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb 모델 생성\n",
    "xgb = XGBClassifier(learning_rate=0.1, \n",
    "                    n_estimators=2000,\n",
    "                    max_depth=3,\n",
    "                    min_child_weight=3,\n",
    "                    gamma=0, \n",
    "                    subsample=0.8, # 각 트리마다의 관측 데이터 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    colsample_bytree=0.8, # 각 트리마다의 feature 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    objective= 'binary:logistic', \n",
    "                    verbose=10)\n",
    "\n",
    "# parameter 들을 dictionary 형태로 설정\n",
    "xgb_params = { \n",
    "              'gamma':[i/10.0 for i in range(0,5)]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# GridSearch를 통해 최적 hyperparameter를 검색\n",
    "clf = GridSearchCV(xgb,xgb_params,scoring='accuracy',cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "pred = clf.predict(x_test)\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,pred)))\n",
    "end = time.time()\n",
    "print(\"XGB 수행 시간: {0:.1f} 초 \".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb 모델 생성\n",
    "xgb = XGBClassifier(learning_rate=0.1, \n",
    "                    n_estimators=2000,\n",
    "                    max_depth=3,\n",
    "                    min_child_weight=3,\n",
    "                    gamma=0, \n",
    "                    subsample=0.8, # 각 트리마다의 관측 데이터 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    colsample_bytree=0.8, # 각 트리마다의 feature 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                    objective= 'binary:logistic', \n",
    "                    verbose=10)\n",
    "\n",
    "# parameter 들을 dictionary 형태로 설정\n",
    "xgb_params = { \n",
    "              'subsample':[i/10.0 for i in range(6,10)],\n",
    "              'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# GridSearch를 통해 최적 hyperparameter를 검색\n",
    "clf = GridSearchCV(xgb,xgb_params,scoring='accuracy',cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "pred = clf.predict(x_test)\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,pred)))\n",
    "end = time.time()\n",
    "print(\"XGB 수행 시간: {0:.1f} 초 \".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456724b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b020aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dade22b7",
   "metadata": {},
   "source": [
    "## 3. Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=2000)\n",
    "evals = [(x_test,y_test)]\n",
    "\n",
    "start = time.time()\n",
    "# 학습 : 조기중단 수행(100)\n",
    "lgbm_wrapper.fit(x_train,y_train,early_stopping_rounds=100,\n",
    "                eval_metric='logloss',eval_set=evals, verbose=True)\n",
    "\n",
    "# 예측\n",
    "preds = lgbm_wrapper.predict(x_test)\n",
    "pred_probs = lgbm_wrapper.predict_proba(x_test)[:, 1]\n",
    "end = time.time()\n",
    "print(\"LGBM 수행 시간: {0:.1f} 초 \".format(end - start))\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ce3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test,pred)\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,recall_score,precision_score\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test.values, pred_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='red',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8ba1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_importance( )를 이용하여 feature 중요도 시각화\n",
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(lgbm_wrapper, ax=ax,importance_type='split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108138e",
   "metadata": {},
   "source": [
    "#### Grid Search를 이용한 최적 하이퍼 파라미터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d91300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# lgmb 모델 생성\n",
    "lgb = LGBMClassifier(learning_rate=0.1, \n",
    "                       n_estimators=1000,\n",
    "                       subsample=0.8, # 각 트리마다의 관측 데이터 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                       colsample_bytree=0.8, # 각 트리마다의 feature 샘플링 비율, default = 1, 일반적으로 0.5 ~ 1\n",
    "                       verbose=10)\n",
    "\n",
    "# parameter 들을 dictionary 형태로 설정\n",
    "lgb_params = { \n",
    "              'num_leaves':[20,40,60,80,100], #  num_leaves = 2^(max_depth)는 depth-wise tree와 같은 수의 leaves를 가지게 하여, 이보다 작게 설정해야 오버피팅을 줄일 수 있다.\n",
    "              'max_depth':range(3,13,3), \n",
    "              'min_child_samples':range(1,20,5),\n",
    "              'min_child_weight':range(1,6,2),\n",
    "            \n",
    "             }\n",
    "# _params = {'num_leaves':[20,40,60,80,100], \n",
    "#               'min_child_samples':[5,10,15],\n",
    "#               'max_depth':[-1,5,10,20],\n",
    "#               'learning_rate':[0.05,0.1,0.2],\n",
    "#               'reg_alpha':[0,0.01,0.03]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ac12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# GridSearch를 통해 최적 hyperparameter를 검색\n",
    "clf = GridSearchCV(lgb,lgb_params,scoring='accuracy',cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "pred = clf.predict(x_test)\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,pred)))\n",
    "end = time.time()\n",
    "print(\"LGBM 수행 시간: {0:.1f} 초 \".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c66793",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_wrapper = LGBMClassifier(n_estimators=2000,max_depth=3,min_child_smaples=1,min_child_weight=5,num_leaves=20,reg_alpha=0)\n",
    "evals = [(x_test,y_test)]\n",
    "\n",
    "start = time.time()\n",
    "# 학습 : 조기중단 수행(100)\n",
    "lgbm_wrapper.fit(x_train,y_train,early_stopping_rounds=100,\n",
    "                eval_metric='logloss',eval_set=evals, verbose=True)\n",
    "\n",
    "# 예측\n",
    "preds = lgbm_wrapper.predict(x_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(x_test)[:, 1]\n",
    "end = time.time()\n",
    "print(\"LGBM 수행 시간: {0:.1f} 초 \".format(end - start))\n",
    "print('분류 결과 : {0:.1f} '.format(accuracy_score(y_test,preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test,pred)\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2dae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac930e",
   "metadata": {},
   "source": [
    "#### LGBM scaler 별 결과\n",
    "- Standard Scaler : 수행 시간 =  초, 정확도 = 1.0\n",
    "- Robust Scaler : 수행 시간 =  초, 정확도 = 1.0\n",
    "- Min Max Scaler : 수행 시간 = 3.9 초, 정확도 = 1.0\n",
    "\n",
    "#### 각 Scaler 별 최적 하이퍼 파라미터 수치\n",
    "- Standard Scaler : \n",
    "- Robust Scaler : max_depth=10, min_child_smaples=10, num_leaves=40, reg_alpha=0\n",
    "- Min Max Scaler : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c0e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515d309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9271e414be5e055cabef0148537efe95905a2cbc3a51060d18455594802bc000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
